{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3787ea19",
   "metadata": {},
   "source": [
    "# edX Natural Language Processing Foundations – Assignment 1: Regular Expression & Text Preprocessing\n",
    "\n",
    "Hello everyone, this assignment notebook covers **Regular Expressions** and **Text Preprocessing**. There are some code-completion tasks in this notebook. For each code completion task, please write down your answer (i.e., your lines of code) between sentences that `Your code starts here` and `Your code ends here`. The space between these two lines does not reflect the required or expected lines of code.\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:**\n",
    "* Remember to rename and save this Jupyter notebook as **A1_edXusername.ipynb** (e.g., **A1_bobsmith.ipynb**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Remember to rename and save the script file **A1_script.py** as **A1_edXusername.py** (e.g., **A1_bobsmith.py**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* **Do not change any predefined variable names!** For example in Task 1.1 a), you need to find the correct RegEx as value for the variable named `pattern_11a`; do not change this variable name!\n",
    "\n",
    "Please also add your name and edX id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38b136-64a1-4f09-8227-fbeb6585e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "edx_username = ''  # e.g., bobsmith, you can check this in edX `Account Settings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bef3a",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts are there to provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well marked, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Regular Expressions (20 Points)**\n",
    "    * 1.1 Basic Expressions (10 Points)\n",
    "        * 1.1 a) Expression 1 (2 Points)\n",
    "        * 1.1 b) Expression 2 (2 Points)\n",
    "        * 1.1 c) Expression 3 (3 Points)\n",
    "        * 1.1 d) Expression 4 (3 Points)\n",
    "    * 1.2 Regular Expressions & Finite State Automata (10 Points)\n",
    "        * 1.2 a) FSA 1 (2 Points)\n",
    "        * 1.2 b) FSA 2 (2 Points)\n",
    "        * 1.2 c) FSA 3 (3 Points)\n",
    "        * 1.2 c) FSA 4 (3 Points)\n",
    "* **2 Minimum Edit Distance (10 Points)**\n",
    "    * 2.1 a) MED 1 (5 Points)\n",
    "    * 2.1 b) MED 2 (5 Points)\n",
    "* **3 Text Preprocessing (with RegEx) (20 Points)**\n",
    "    * 3.1 Tokenization (10 Points)\n",
    "    * 3.2 Feature Extraction (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c454e2",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481d54b",
   "metadata": {},
   "source": [
    "#### Required Imports\n",
    "\n",
    "For this notebook, we only need the 2 in-built packages `re` and `json` (in fact, the latter is only used for printing some outputs more nicely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb63e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41803613",
   "metadata": {},
   "source": [
    "#### Utility Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792cd20-cfd8-4895-8543-7f2f16d2e13c",
   "metadata": {},
   "source": [
    "The method `show_matches()` offers a very simple way to print the result of a substring matching using a Regular Expression. For a given input text and RegEx pattern it returns all matches separated by the pipe symbol (`|`) a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ca0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matches(pattern, string, flags=0):\n",
    "    # Compile RegEx pattern\n",
    "    p = re.compile(pattern, flags=flags)\n",
    "    # Match pattern against input text\n",
    "    matches = list(p.finditer(string))\n",
    "    # Handle matches\n",
    "    if len(matches) == 0:\n",
    "        print(\"No match found.\", \"\\n\")\n",
    "    else:\n",
    "        print(' | '.join([ m.group() for m in matches] ), '\\n')\n",
    "            \n",
    "show_matches(r\"[\\w]+\", \"Welcome to the assignment for Section 2 of NLP Foundations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ae408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23782f7",
   "metadata": {},
   "source": [
    "## 1 Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f7236",
   "metadata": {},
   "source": [
    "### 1.1 Basic Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bce31",
   "metadata": {},
   "source": [
    "#### 1.1 a) Match all user mentions and hashtags!\n",
    "\n",
    "In a Tweet, a user mention is a string starting with `@` and is used to address a Twitter user (more generally: account). A hashtag start with `#` and is used to  link to all of the other Tweets that also include that hashtag. You can assume that a user mention and hashtag can only contain characters and digits. There are some additional rules, but ignore them here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed52602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11a = r\"\"\n",
    "  \n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11a, \"I just sent my #jobapplication to @NUSComputing. Wish me luck! #motivated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f3783",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "#jobapplication | @NUSComputing | #motivated\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b08b0e",
   "metadata": {},
   "source": [
    "#### 1.1 b) Match all word-like expressions of laughter!\n",
    "\n",
    "Particularly on social media, users make use of special \"words\" to express emotions. Identifying such words is very useful for sentiment analysis. In this task, your goal is to identify special words that are commonly used to express laughter. More precisely, you are supposed to match the following cases\n",
    "\n",
    "* haha, hahaha, hahahaha, etc.\n",
    "* hehe, hehehe, hehehehe, etc.\n",
    "* xixi, xixixi, xixixixi, etc.\n",
    "\n",
    "All other variants should *not* be matched. The capitalization should not matter; we already set the flag for this in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c66bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11b = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11b, \"Hehehe...my dog just caught his tail! haha #justpetthings\", flags=re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545a7d3",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "Hehehe | haha \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2d057",
   "metadata": {},
   "source": [
    "#### 1.1 c) Match emoticons\n",
    "\n",
    "An even more common way in social media to express emotions is the use of so-called *emoticons*. Emoticons are punctuation marks, letters, and numbers used to create pictorial icons that generally display an emotion or sentiment. Matching emoticons is therefore also very useful for sentiment analysis.\n",
    "\n",
    "For simplification, we focus \"Western emoticons\" which are mostly written from left to right as though the head is rotated counterclockwise 90 degrees, e.g., `:-|` or `;o)`. We also assume that each emoticon consists of an eye part (`:` or `;`) and optional nose part (`-`, `o`), and a mouth part (`)`, `(`, `|`, `p`). You can assume that no part is repeated -- that is, in cases such as `:-)))`, it is sufficient to match only `:-)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d55c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11c = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "show_matches(pattern_11c, \"First I was :( but then then I was all :o)))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb393f5c",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    ":( | :o) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2bc18",
   "metadata": {},
   "source": [
    "#### 1.1 d) Extract core content from HTML web pages!\n",
    "\n",
    "Raw HTML content from web pages are a very common basis for a text corpus. The challenge is that the HTML code of a whole page often contains elements that are not part of the actual content (e.g., header, footer, navigation bar, side bar, etc.). A very common way to indicate the \"real\" content is to use the `<p>` tag to define paragraphs -- that is, anything between the opening `<p>` tag and the closing `</p>` tag can be assumed to be part of the main content.\n",
    "\n",
    "Write a Regular Expression that matches the content between these 2 tags, but do not match the tags themselves! To keep it simple, you can assume that the `<p>` tags are not nested. For example, cases such as `<p>This is <p>a</p> test.</p>` won't occur. In fact, this would be kind of invalid HTML and would be ignored by the browser (check [here](https://www.w3.org/TR/html401/struct/text.html#h-9.3.1) if you are interested in the rules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f93357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "pattern_11d = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   \n",
    "\n",
    "html = \"\"\"\n",
    "    <div class='article'>\n",
    "        <h2>Sleep More!</h2>\n",
    "        <p>A new study has shown than sleep is very important.</p>\n",
    "        <p>The authors survey 160 scientific paper for a meta analysis.</p>\n",
    "        <p>Their findings will be published in the next issue of Nature.</p>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "show_matches(pattern_11d, html, flags=re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86adf64",
   "metadata": {},
   "source": [
    "The expected output for the code cell above is:\n",
    "\n",
    "```\n",
    "A new study has shown than sleep is very important. | The authors survey 160 scientific paper for a meta analysis. | Their findings will be published in the next issue of Nature. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018ea88",
   "metadata": {},
   "source": [
    "### 1.2 Regular Expressions & Finite State Automata\n",
    "\n",
    "We saw that a Regular Expression describe **Regular Language** which is a language accepted by a Finite State Automaton (FSA). This means that each Regular Expression can be represented by an FSA, and vice versa. In fact, this mapping might not even be unique -- that is, the same Regular Expression can be represented by different FSA and vice versa. In short, there is generally no unique solution for all subtasks below.\n",
    "\n",
    "In the following, you are given 4 FSA, and your task is to find a Regular Expression that matches each FSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbe6a8",
   "metadata": {},
   "source": [
    "#### 1.2 a) Given is the Finite State Automata below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b71aa0-32bb-4450-93e9-5b1e1e0631a7",
   "metadata": {},
   "source": [
    "<img src=\"data/fsm1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bb9ba",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12a = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365cbf4",
   "metadata": {},
   "source": [
    "#### 1.2 a) Given is the Finite State Automata below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc69ef-b018-49be-bfd7-04ff1b4d9226",
   "metadata": {},
   "source": [
    "<img src=\"data/fsm2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cedf8",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12b = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73adcae-4819-4bdd-a83f-5c1cbd7a0897",
   "metadata": {},
   "source": [
    "#### 1.2 c) Given is the Finite State Automata below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98443f1a-7ba6-4ee7-9802-ae54a9d6a3c2",
   "metadata": {},
   "source": [
    "<img src=\"data/fsm3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875efca3-897b-4816-8f88-79577e1f8dbf",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9db15-e2d7-4c5b-8c0f-45cf8a892c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12c = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cb2da-ce7a-413a-a826-94a3c1fa6bf7",
   "metadata": {},
   "source": [
    "#### 1.2 d) Given is the FSA below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c92aa9-d80e-4222-b2e9-c9871f7eb490",
   "metadata": {},
   "source": [
    "<img src=\"data/fsm4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e48ee-06ae-4fc1-9c65-c823cce24e07",
   "metadata": {},
   "source": [
    "Find a Regular Expression that matches the FSA above and enter it in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892872ea-e4eb-4fd4-bf82-ca99f9571dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "regex_12d = r\"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f5678",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c71f4e",
   "metadata": {},
   "source": [
    "## 2 Minimum Edit Distance\n",
    "\n",
    "Minimum Edit Distance (MED) is an important metric to calculate the difference between 2 strings.\n",
    "\n",
    "In our videos, we gave the example using the 2 words *LANGUAGE* and *SAUSAGE* how their MED is calculated using a Dynamic Programming approach. We visualized this process of computing the MED using the tables below, showing two things:\n",
    "\n",
    "* The MED between *LANGUAGE* and *SAUSAGE*, which is 5 (indicated be the top-right tables cell)\n",
    "\n",
    "* The \"path\" from start to finish indicating the **INSERT**, **DELETE**, and **SUBSTITUTE** operation need to get the MED (indicated by the blue table cells)\n",
    "\n",
    "We assume that the cost for the operations are: **INSERT=1**, **DELETE=1**, **SUBSTITUTE=2**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46632c84",
   "metadata": {},
   "source": [
    "<img src=\"data/med-language-sausage.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357908c6",
   "metadata": {},
   "source": [
    "Apart from the final MED value, the table also gives an alignment. Recall that there are many possible alignments, but we are only interested in alignments that yield the MED (still, there can be more than one). On the slides, we used the alignment shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c9bd6",
   "metadata": {},
   "source": [
    "<img src=\"data/med-alignment-language-sausage.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbbf13",
   "metadata": {},
   "source": [
    "In this task, you will perform these calculation for 2 simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b1ef2",
   "metadata": {},
   "source": [
    "### 2.1 Calculating Minimum Edit Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ab6fe",
   "metadata": {},
   "source": [
    "#### 2.1 a)  MED for Spelling Correction\n",
    "\n",
    "We first consider a common case for typos when writing a text: the accidental swapping of letters in a word. Most of the time, people swap just one pair of letters, but here we made it a bit more interesting. The two words we compare are *TIMES* and *ITEMS*\n",
    "\n",
    "**TASK:** Calculate the Minimal Edit Distance (MED) between *TIMES* and *ITEMS* and give an alignment yielding this MED! You are given the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960b668",
   "metadata": {},
   "source": [
    "<img src=\"data/med-times-items.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda90ee2",
   "metadata": {},
   "source": [
    "Complete the table above to calculate MED between *TIMES* and *ITEMS*. You can complete the table using pen and paper; there is no need to submit the fully completed table. Completing the table will give you the 3 values for **A**, **B**, and **C**. Assign these 3 values to the respective variables in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "med_21a_A = -1\n",
    "med_21a_B = -1\n",
    "med_21a_C = -1\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd1e882",
   "metadata": {},
   "source": [
    "Give an alignment between *TIMES* and *ITEMS* yielding the MED using the code cell below. The example for *LANGUAGE* and *SAUSAGE* illustrates the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example format for alignment\n",
    "#med_language = \"LANGU*AGE\"\n",
    "#med_sausage  = \"SAU**SAGE\"\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "med_alignment_21a_times = \"\"\n",
    "med_alignment_21a_items = \"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4db1d",
   "metadata": {},
   "source": [
    "#### 2.1 b)  MED for Voice Recognition (Speech-to-Text)\n",
    "\n",
    "Many words, particularly when not pronounced very clearly or with a lot of surrounding noise, can sound very similar. This can easily lead to misidentified words in speech-to-text systems; think about Apple's Siri or Amazon's Alexa. In our example we consider the two words *GRAMMAR* and *GRANDMA*. If these words are not clearly pronounced, they can sound very similar. Try it for yourself!\n",
    "\n",
    "**TASK:** Calculate the Minimal Edit Distance (MED) between *GRAMMAR* and *GRANDMA* and give an alignment yielding this MED! You are given the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78830f68",
   "metadata": {},
   "source": [
    "<img src=\"data/med-grammar-grandma.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69620f5",
   "metadata": {},
   "source": [
    "Complete the table above to calculate MED between *GRAMMAR* and *GRANDMA*. You can complete the table using pen and paper; there is no need to submit the fully completed table. Completing the table will give you the 3 values for **A**, **B**, and **C**. Assign these 3 values to the respective variables in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "med_21b_A = -1\n",
    "med_21b_B = -1\n",
    "med_21b_C = -1\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371627a5",
   "metadata": {},
   "source": [
    "Give an alignment between *GRAMMAR* and *GRANDMA* yielding the MED using the code cell below. The example for *LANGUAGE* and *SAUSAGE* illustrates the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example format for alignment\n",
    "#med_language = \"LANGU*AGE\"\n",
    "#med_sausage  = \"SAU**SAGE\"\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################  \n",
    "\n",
    "med_alignment_21b_grammar = \"\"\n",
    "med_alignment_21b_grandma = \"\"\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabff724",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389e1e0",
   "metadata": {},
   "source": [
    "## 3 Text Preprocessing (with RegEx)\n",
    "\n",
    "We covered a series of common preprocessing steps performed over raw text. Many of them are very simple to implement. For example, Python (but arguably all modern programming languages) come with in-built methods to lowercase or uppercase a string; see the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a TEST to see how SiMpLe case-folding a STRING is.\".lower()\n",
    "#text = \"This is a TEST to see how SiMpLe case-folding a STRING is.\".upper()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc800144",
   "metadata": {},
   "source": [
    "Another common and easy-to-implement preprocessing step is stopword removal. In the code cell below, we make 2 assumptions\n",
    "\n",
    "* We have a list of stopwords we want to remove (such list a widely available, and part of many NLP toolkits)\n",
    "* Our text has already been tokenized -- we address the challenge of tokenization in more detail below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined set of stopwords (very simplified) -- note that all are lowercase\n",
    "stopwords = [\"a\", \"an\", \"the\", \"and\", \"but\", \"to\", \"be\", \"is\", \"'s'\", \"was\", \"not\", \"i\", \"me\", \"my\"]\n",
    "\n",
    "# Tokenized input text\n",
    "tokens = [\"I\", \"like\", \"to\", \"study\", \"NLP\", \".\"]\n",
    "\n",
    "# Remove stopwords using list comprehension\n",
    "tokens = [ t for t in tokens if t.lower() not in stopwords ]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e43095",
   "metadata": {},
   "source": [
    "Since case-folding and stopword removal does not pose much of a challenge, we don't ask you to implement these steps.\n",
    "\n",
    "In contrast, we saw that other common preprocessing steps are much less straightforward, particularly stemming and lemmatization. While rule and lookup-based approaches are conceptually not very difficult, their actual implementation requires a certain amount of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ab3e8",
   "metadata": {},
   "source": [
    "## 3.1 Tokenization\n",
    "\n",
    "We saw that tokenization is one of the most import preprocessing steps. The goal of tokenization is to split a string into tokens, where a token is sequence of characters with a semantic meaning (typically: words, numbers, punctuation -- but may differ depending on the application).\n",
    "\n",
    "Your task is to implement a simple but still practical tokenizer. There are different approaches to this task. For this assignment, we follow the basic 2-part approach of the spaCy tokenizer as already covered on our slides:\n",
    "\n",
    "* Split the input string w.r.t. whitespace characters (to get the initial set of tokens)\n",
    "* From left to right, recursively check each token if it should be split into further subtokens\n",
    "\n",
    "The screenshot taken from the [spaCy website](https://spacy.io/usage/spacy-101#annotations-token) illustrates the idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b546e3b",
   "metadata": {},
   "source": [
    "<img src=\"data/spacy-tokenizer-example.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f6e06",
   "metadata": {},
   "source": [
    "As you can see from the illustration, if a token gets split, the resulting subtokens have to be checked again until no further splitting is possible. Only then the tokenizer can move on to the current token list. The method `tokenize()` already implements this idea, and you don't have to worry about that.\n",
    "\n",
    "The heart of this method is the call of method `split_token()` which checks if a token needs to be split or not (e.g., *Let's* into *Let* and *'s*). If indeed a split is required, `split_token()` splits the token into 2 or more subtokens -- that's kind of up to you -- and returns the new subtokens as a list.\n",
    "\n",
    "Have a good look at method `tokenize()` and convince yourself that it implements the basic approach of the spaCy tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "\n",
    "    # Split by whitespace\n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    pos = 0\n",
    "    while True:\n",
    "        # Stop if we reach the end of the token list\n",
    "        if pos >= len(tokens):\n",
    "            break\n",
    "\n",
    "        # Grab the next token\n",
    "        token = tokens[pos]\n",
    "\n",
    "        # Split the token into 2 or more subtokens; might not be necessary!\n",
    "        components = split_token(token)\n",
    "\n",
    "        # Just for safety, we ignore any empty component\n",
    "        # (this might happen during the split and it's more convenient to handle it here)\n",
    "        components = [ c.strip() for c in components if len(c.strip()) > 0]\n",
    "        \n",
    "        # Check if the current token was indeed split into 2 more components\n",
    "        if len(components) > 1:\n",
    "            # If the token was split create a new token list\n",
    "            tokens = tokens[:pos] + components + tokens[pos+1:]\n",
    "        else:\n",
    "            # If the token was NOT split; just go to the next token in the list\n",
    "            pos +=1\n",
    "\n",
    "    # Return final list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb379cb0",
   "metadata": {},
   "source": [
    "Your task is now to implement the method `split_token()` in the code below. The method takes a token as input -- note that a token can never contain any whitespace character -- and decides if the token needs to be split according to the given tokenization rules. There a variety of such rules -- and different off-the-shelf tokenizers may differ in their set of rules -- but for this task focus only on the following:\n",
    "\n",
    "* **Basic punctuation:** The most common limitation of tokenizing only using whitespace characters is that there is no whitespace between a word token and a common punctuation mark. So we need to split this. For this, we provide you with the solution -- see the `split_token()` method below -- to illustrate how to use the method and what needs to be returned.\n",
    "* **Ellipsis:** In informal writing, an ellipsis (`...`) can be used to represent a trailing off of thought (e.g., *\"If only she had . . . Oh, it doesn’t matter now.\"*) or also indicate hesitation (e.g., *\"I wasn’t really . . . well, what I mean . . . see, the thing is . . . I didn’t mean it.\"*), though in this case the punctuation is more accurately described as suspension points. You can assume that each ellipsis is correctly represented by 3 periods (`...`), no more no less. While some style guides suggest having a whitespace character between words and `...`, these whitespace characters are not mandatory and often omitted. So we have to take this into consideration.\n",
    "* **Clitics:** A clitic is a morpheme that has syntactic characteristics of a word, but depends phonologically on another word or phrase. A clitic is pronounced like an affix, but plays a syntactic role at the phrase level. For example, the contracted forms of the auxiliary verbs in *I'm* and *we've* are clitics. The negative marker *n't* as in *couldn't* etc. is typically considered a clitic. In line with most tokenizers, we also want to split clitics from the preceding word.\n",
    "* **Hyphenated words:** Most tokenizers (incl. the spaCy tokenizer) splits hyphenated words such as *long-lived*, *old-fashioned*, or *mother-in-law*. Hence, our tokenizer should do the same.\n",
    "\n",
    "\n",
    "**Comment & Hints**\n",
    "* All rules can be implemented using Regular Expression in a rather straightforward manner. However, you are not required to use Regular Expression for this task. For example, you can look into basic methods provided by Python such as [`split()`](https://www.w3schools.com/python/ref_string_split.asp), [`startswidth()`](https://www.w3schools.com/python/ref_string_startswith.asp) or [`endswith()`](https://www.w3schools.com/python/ref_string_endswith.asp).\n",
    "* Once any rule requires the split of a token, you can immediately return `components` -- see the part for handling the splitting of basic punctuation marks in `split_token()`. There is no need to check the other rules, as the loop in `tokenize()` takes care that all new subtokens get checked again if a further splitting might be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fdaac-1ebf-402f-a3a6-70ba81657f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(token):\n",
    "    \n",
    "    # Contains all the subtokens that might result from any splitting\n",
    "    components = []\n",
    "    \n",
    "    # Common Punctuation marks (word followed by a single punctuation mark)\n",
    "    for match in re.finditer(r\"(\\w+)([.:;?!]{1})\", token):\n",
    "        components.append(token[:match.span(1)[1]])\n",
    "        components.append(token[match.span(1)[1]:])\n",
    "        return components    \n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################    \n",
    "\n",
    "    # Ellipsis\n",
    "\n",
    "    \n",
    "    # Clitics\n",
    "        \n",
    "    \n",
    "    # Hyphenation: split the words concatenated by hypen\n",
    "\n",
    "    \n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################    \n",
    "    \n",
    "    # If we didn't find any reason to split, return the token as the only subtoken itself\n",
    "    return [token]\n",
    "\n",
    "        \n",
    "#tokenize(\"Let's ask ourselves: Isn't 13.5 dollars too much?!?!? :o)\")\n",
    "tokenize(\"It's a free-for-all market...you should've invested ...! Too bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f03630-e220-42f8-aa75-a8cd1a52d5d3",
   "metadata": {},
   "source": [
    "For the given example text, the expeact outcome is as follows:\n",
    "\n",
    "```\n",
    "['It',\n",
    " \"'s\",\n",
    " 'a',\n",
    " 'free',\n",
    " '-',\n",
    " 'for',\n",
    " '-',\n",
    " 'all',\n",
    " 'market',\n",
    " '...',\n",
    " 'you',\n",
    " 'should',\n",
    " \"'ve\",\n",
    " 'invested',\n",
    " '...',\n",
    " '!',\n",
    " 'Too',\n",
    " 'bad',\n",
    " '!']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3c81e",
   "metadata": {},
   "source": [
    "### 3.2 Feature Extraction\n",
    "\n",
    "With the task of **Feature Extraction** we already look ahead a bit to Section 4, but this does not matter here. The problem is that many fundamental algorithms for NLP tasks assume numerical inputs with a fixed length. A text, however, is a variable sequence of words. We therefore have to transform our input text into such a numerical representation -- again, we discuss it in much more detail in Section 4.\n",
    "\n",
    "With our current knowledge, we can already look at a very basic approach: the extraction of a fixed set of numerical characteristics (i.e., *features*) from an input text. The choice of features depends on the applications tasks. In the following, we assume that we want build a **Sentiment Analysis** system to decide if a text is either \"positive\" or \"negative\". For this, we want to extract the following features:\n",
    "\n",
    "* Total length of the input text\n",
    "* Number of non-whitespace characters in the text\n",
    "* Number of uppercase words (e.g., \"NICE\", \"HAPPY\")\n",
    "* Number of \"positive\" words (e.g., \"happy\", \"great\")\n",
    "* Number of \"negative\" words (e.g., \"lonely\", \"sad\")\n",
    "* Number of exclamation marks\n",
    "\n",
    "Regarding the list of \"positive\" and \"negative\" words, there are many comprehensive word list publicly available only for download. To keep it simple, we assume the following two lists with \"positive\" and \"negative\" words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDLIST_POSITIVE = ['happy', 'great', 'nice', 'awesome', 'joyful']\n",
    "WORDLIST_NEGATIVE = ['sad', 'horrible', 'boring', 'lonely', 'cruel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ba63a",
   "metadata": {},
   "source": [
    "Your task is to implement the method `extract_features()` below. To illustrate the basic idea, we already provide you with a solution for calculating the number of emoticons -- because you have already done this in Task 1 :). Note that a proper Sentiment Analysis system should distinguish between positive emoticons (e.g., `:-)`) and negative emoticons (e.g., `:-(`). However, this is a rather straightforward extension, so we ignore it here and treat all emoticons the same.\n",
    "\n",
    "**Comments & Hints:**\n",
    "\n",
    "* All features can be extracted using Regular Expression. However, you can use an in-built method provided by Python to solve this task.\n",
    "* We recommend always using `flags=re.I` or `flags=re.IGNORECASE` to ensure that all your matches are insensitive to the case of the words. If you don't use Regular Expression you need to handle the case of words differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    \n",
    "    features = {\n",
    "        \"text_length\": 0,              # total length of text incl. all characters\n",
    "        \"num_nonwhite_characters\": 0,  # number of non-whitespace characters\n",
    "        \"num_uppercase_words\": 0,      # number of words in all uppercase (e.g., \"NICE\", \"HAPPY\")\n",
    "        \"num_positive_words\": 0,\n",
    "        \"num_negative_words\": 0,\n",
    "        \"num_exclamation_marks\": 0,\n",
    "        \"num_emoticons\": 0\n",
    "    }\n",
    "    \n",
    "    features[\"num_emoticons\"] = len(list(re.finditer(pattern_11c, text, flags=re.I)))\n",
    "    \n",
    "    #########################################################################################\n",
    "    ### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ### Your code ends here #################################################################\n",
    "    #########################################################################################\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "features = extract_features(\"Nice!!! This makes me both HAPPY and SAD! :)\")\n",
    "\n",
    "print(json.dumps(features, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f502d-1de4-4b17-aacd-abaf41685836",
   "metadata": {},
   "source": [
    "For the given example text, the expeact outcome is as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"text_length\": 44,\n",
    "  \"num_nonwhite_characters\": 36,\n",
    "  \"num_uppercase_words\": 2,\n",
    "  \"num_positive_words\": 2,\n",
    "  \"num_negative_words\": 1,\n",
    "  \"num_exclamation_marks\": 4,\n",
    "  \"num_emoticons\": 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e54f8-b34c-4750-87ee-44beb267c25b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This assignment tested you on working with strings, which represent the basic representation of text data. One the hand, basic NLP tasks such as substring matching using Regular Expression are a very useful tool in practice. On the other hand, for more sophisticated NLP tasks, we typically need to convert a text as a sequence of characters into meaningful units (typically words). Again this can be done using Regular Expressions.\n",
    "\n",
    "Lastly, we briefly looked into Feature Extraction in terms of extracting numerical values to reflect meaningful characteristics from a text. We will cover the need for this in more detail in Section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b88c87-92a1-4285-a313-e9d2f052d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
